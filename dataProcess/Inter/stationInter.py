import glob
import json
import os
import time

import numba
import numpy as np
import pandas as pd
import xarray as xr
from filelock import FileLock
from joblib import Parallel, delayed, parallel_backend
from pykrige.ok import OrdinaryKriging
from pykrige.uk import UniversalKriging
from scipy import ndimage
from scipy.spatial.distance import cdist
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from tqdm.auto import tqdm


@numba.njit(parallel=True)
def calculate_distances_and_weights(col_lats, col_lons, valid_lats, valid_lons):
    """è®¡ç®—è·ç¦»çŸ©é˜µå’Œé›¶è·ç¦»æ©ç çš„NumbaåŠ é€Ÿå‡½æ•°"""
    n_points = len(col_lats)
    n_stations = len(valid_lats)

    # åˆå§‹åŒ–è·ç¦»çŸ©é˜µ
    distances = np.zeros((n_points, n_stations), dtype=np.float32)
    zero_dist_mask = np.zeros((n_points, n_stations), dtype=np.bool_)

    # å¹¶è¡Œè®¡ç®—æ‰€æœ‰è·ç¦»
    for i in numba.prange(n_points):
        for j in range(n_stations):
            lat_diff = col_lats[i] - valid_lats[j]
            lon_diff = col_lons[i] - valid_lons[j]
            dist = np.sqrt(lat_diff**2 + lon_diff**2)
            distances[i, j] = dist
            zero_dist_mask[i, j] = dist < 1e-10

    # è®¡ç®—é›¶è·ç¦»å’Œéé›¶è·ç¦»æ©ç 
    has_zero_dist = np.zeros(n_points, dtype=np.bool_)
    for i in range(n_points):
        for j in range(n_stations):
            if zero_dist_mask[i, j]:
                has_zero_dist[i] = True
                break

    return distances, zero_dist_mask, has_zero_dist


@numba.njit
def handle_zero_distances(idx, zero_dist_mask, valid_values, n_stations):
    """å¤„ç†é›¶è·ç¦»ç‚¹çš„NumbaåŠ é€Ÿå‡½æ•°"""
    station_count = 0
    value_sum = 0.0

    for j in range(n_stations):
        if zero_dist_mask[idx, j]:
            value_sum += valid_values[j]
            station_count += 1

    if station_count > 0:
        return value_sum / station_count
    else:
        return 0.0


@numba.njit
def calculate_idw_weights(distances, power=2.0):
    """è®¡ç®—IDWæƒé‡çš„NumbaåŠ é€Ÿå‡½æ•°"""
    weights = 1.0 / (distances**power + 1e-10)
    # ä¸ä½¿ç”¨keepdimså‚æ•°ï¼Œæ‰‹åŠ¨reshape
    weights_sum = np.sum(weights, axis=1)
    # æ‰‹åŠ¨è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…åŸå§‹è¡Œä¸º
    weights_sum = weights_sum.reshape(weights_sum.shape[0], 1)
    return weights / weights_sum


@numba.njit
def predict_linear_model(elevs, coef, intercept):
    """çº¿æ€§æ¨¡å‹é¢„æµ‹çš„NumbaåŠ é€Ÿå‡½æ•°"""
    return coef * elevs + intercept


@numba.njit(parallel=True)
def process_grid_column(
    col_lats,
    col_lons,
    col_elevs,
    col_regions,
    valid_lats,
    valid_lons,
    valid_values,
    region_coefs,
    region_intercepts,
    station_residuals,
    n_regions,
    has_model,
):
    """å¤„ç†å•åˆ—ç½‘æ ¼ç‚¹çš„NumbaåŠ é€Ÿå‡½æ•°"""
    n_points = len(col_lats)
    n_stations = len(valid_lats)
    result = np.zeros(n_points, dtype=np.float32)

    # è®¡ç®—è·ç¦»çŸ©é˜µ
    distances, zero_dist_mask, has_zero_dist = calculate_distances_and_weights(
        col_lats, col_lons, valid_lats, valid_lons
    )

    # å¤„ç†é›¶è·ç¦»ç‚¹
    for i in range(n_points):
        if has_zero_dist[i]:
            result[i] = handle_zero_distances(i, zero_dist_mask, valid_values, n_stations)

    # è®¡ç®—éé›¶è·ç¦»ç‚¹çš„IDWæƒé‡
    non_zero_mask = ~has_zero_dist
    if np.any(non_zero_mask):
        # è®¡ç®—IDWæƒé‡
        idw_weights = calculate_idw_weights(distances[non_zero_mask])

        # å¤„ç†æ¯ä¸ªåŒºåŸŸ
        for r in range(n_regions):
            # æ‰¾å‡ºå½“å‰åŒºåŸŸçš„ç½‘æ ¼ç‚¹
            region_points_mask = np.zeros(n_points, dtype=np.bool_)
            for i in range(n_points):
                if non_zero_mask[i] and col_regions[i] == r:
                    region_points_mask[i] = True

            if not np.any(region_points_mask):
                continue

            # æ£€æŸ¥è¯¥åŒºåŸŸæ˜¯å¦æœ‰æ¨¡å‹
            if has_model[r]:
                # ä½¿ç”¨æ¨¡å‹é¢„æµ‹å€¼ + IDWæ®‹å·®
                for i in numba.prange(n_points):
                    if region_points_mask[i]:
                        # è·å–è¯¥ç‚¹åœ¨non_zero_maskä¸­çš„ç´¢å¼•
                        non_zero_idx = np.sum(non_zero_mask[:i])

                        # æ¨¡å‹é¢„æµ‹
                        pred_value = predict_linear_model(col_elevs[i], region_coefs[r], region_intercepts[r])

                        # æ®‹å·®IDWæ’å€¼
                        residual_sum = 0.0
                        for j in range(n_stations):
                            residual_sum += idw_weights[non_zero_idx, j] * station_residuals[r, j]

                        # æœ€ç»ˆç»“æœ = æ¨¡å‹é¢„æµ‹ + æ®‹å·®æ’å€¼
                        result[i] = pred_value + residual_sum
            else:
                # æ— æ¨¡å‹ï¼Œä½¿ç”¨ç®€å•IDW
                for i in numba.prange(n_points):
                    if region_points_mask[i]:
                        # è·å–è¯¥ç‚¹åœ¨non_zero_maskä¸­çš„ç´¢å¼•
                        non_zero_idx = np.sum(non_zero_mask[:i])

                        # ç›´æ¥è®¡ç®—IDW
                        value_sum = 0.0
                        for j in range(n_stations):
                            value_sum += idw_weights[non_zero_idx, j] * valid_values[j]

                        result[i] = value_sum

    return result


@numba.njit(parallel=True)
def idw_numba_kernel(grid_points, points, values, p=2, epsilon=1e-10, search_radius=0.0, min_points=1, max_points=0):
    """å¢å¼ºç‰ˆNumbaåŠ é€Ÿçš„IDWæ ¸å¿ƒè®¡ç®—å‡½æ•°

    å‚æ•°:
    grid_points: ç½‘æ ¼ç‚¹åæ ‡æ•°ç»„ [n_grid, 2]
    points: ç«™ç‚¹åæ ‡æ•°ç»„ [n_points, 2]
    values: ç«™ç‚¹è§‚æµ‹å€¼æ•°ç»„ [n_points]
    p: IDWå¹‚æŒ‡æ•°ï¼Œé»˜è®¤ä¸º2
    epsilon: é˜²æ­¢é™¤é›¶çš„å°é‡ï¼Œé»˜è®¤ä¸º1e-10
    search_radius: æœç´¢åŠå¾„ï¼Œ0è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ç«™ç‚¹
    min_points: æœ€å°‘æœ‰æ•ˆç«™ç‚¹æ•°ï¼Œå°‘äºæ­¤æ•°ä½¿ç”¨å…¨éƒ¨ç«™ç‚¹
    max_points: æ¯ä¸ªç½‘æ ¼ç‚¹æœ€å¤šä½¿ç”¨çš„ç«™ç‚¹æ•°ï¼Œ0è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ç«™ç‚¹
    """
    n_grid = len(grid_points)
    n_points = len(points)
    result = np.zeros(n_grid, dtype=np.float32)
    use_radius = search_radius > 0.0
    use_max_points = max_points > 0 and max_points < n_points

    for i in numba.prange(n_grid):
        # åˆå§‹åŒ–è·ç¦»å’Œå€¼æ•°ç»„
        dists = np.zeros(n_points, dtype=np.float32)
        valid_count = 0

        # è®¡ç®—æ‰€æœ‰è·ç¦»
        for j in range(n_points):
            dist = np.sqrt(((grid_points[i, 0] - points[j, 0]) ** 2) + ((grid_points[i, 1] - points[j, 1]) ** 2))
            dists[j] = dist
            # æ£€æŸ¥æ˜¯å¦åœ¨æœç´¢åŠå¾„å†…
            if not use_radius or dist <= search_radius:
                valid_count += 1

        # å¦‚æœæœ‰æ•ˆç‚¹ä¸è¶³ï¼Œå¿½ç•¥åŠå¾„é™åˆ¶
        if valid_count < min_points and use_radius:
            valid_count = n_points

        # å¤„ç†é›¶è·ç¦»ç‚¹
        zero_dist_indices = np.zeros(n_points, dtype=np.int32)
        zero_count = 0

        for j in range(n_points):
            if dists[j] < epsilon:
                zero_dist_indices[zero_count] = j
                zero_count += 1

        # å¦‚æœæœ‰é›¶è·ç¦»ç‚¹ï¼Œç›´æ¥å–å¹³å‡
        if zero_count > 0:
            value_sum = 0.0
            for k in range(zero_count):
                j = zero_dist_indices[k]
                value_sum += values[j]
            result[i] = value_sum / zero_count
            continue

        # é™åˆ¶ä½¿ç”¨ç‚¹æ•°é‡ (ä»…ä¿ç•™æœ€è¿‘çš„max_pointsä¸ªç‚¹)
        if use_max_points and valid_count > max_points:
            # åˆ›å»ºç´¢å¼•æ•°ç»„æ¥æ’åºè·ç¦»
            indices = np.argsort(dists)
            valid_count = max_points
        else:
            # ä½¿ç”¨æ‰€æœ‰ç‚¹ï¼Œæˆ–æœç´¢åŠå¾„å†…çš„ç‚¹
            indices = np.arange(n_points)

        # è®¡ç®—æƒé‡å’ŒåŠ æƒå’Œ
        weight_sum = 0.0
        value_sum = 0.0

        # ä½¿ç”¨æœ‰æ•ˆç‚¹è®¡ç®—IDW
        actual_points = 0
        for k in range(min(valid_count, n_points)):
            j = indices[k]

            # å¦‚æœä½¿ç”¨åŠå¾„é™åˆ¶ï¼Œæ£€æŸ¥è·ç¦»
            if use_radius and dists[j] > search_radius:
                continue

            weight = 1.0 / (dists[j] ** p + epsilon)
            weight_sum += weight
            value_sum += weight * values[j]
            actual_points += 1

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç‚¹ï¼Œä½¿ç”¨å…¨éƒ¨ç‚¹
        if actual_points == 0:
            for j in range(n_points):
                weight = 1.0 / (dists[j] ** p + epsilon)
                weight_sum += weight
                value_sum += weight * values[j]

        # è®¡ç®—æœ€ç»ˆç»“æœ
        result[i] = value_sum / weight_sum

    return result


class PrecipitationInterpolation:
    def __init__(self, station_file, era5_folder, dem_file, output_folder):
        """
        åˆå§‹åŒ–æ’å€¼ç±»

        å‚æ•°:
        station_file: ç«™ç‚¹é™æ°´æ•°æ®æ–‡ä»¶è·¯å¾„
        era5_folder: ERA5é™æ°´æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        dem_file: DEMæ•°æ®æ–‡ä»¶è·¯å¾„
        output_folder: è¾“å‡ºç»“æœæ–‡ä»¶å¤¹è·¯å¾„
        """
        self.station_file = station_file
        self.era5_folder = era5_folder
        self.dem_file = dem_file
        self.output_folder = output_folder

        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
        os.makedirs(output_folder, exist_ok=True)

        # åŠ è½½æ•°æ®
        self.load_data()

    def load_data(self):
        """åŠ è½½ç«™ç‚¹æ•°æ®ã€ERA5æ•°æ®å’ŒDEMæ•°æ®"""
        print("åŠ è½½æ•°æ®...")

        # 1. åŠ è½½ç«™ç‚¹æ•°æ®
        self.station_ds = xr.open_dataset(self.station_file)
        print(f"ç«™ç‚¹æ•°æ®æ—¶é—´èŒƒå›´: {self.station_ds.time.values[0]} åˆ° {self.station_ds.time.values[-1]}")
        print(f"ç«™ç‚¹æ•°é‡: {len(self.station_ds.station)}")

        # 2. æƒ°æ€§åŠ è½½ERA5æ•°æ®ï¼Œä½¿ç”¨chunkså‡å°‘å†…å­˜ä½¿ç”¨
        era5_files = sorted(glob.glob(os.path.join(self.era5_folder, "*.nc")))
        print(f"æ‰¾åˆ° {len(era5_files)} ä¸ªERA5æ–‡ä»¶")
        self.era5_ds = xr.open_mfdataset(
            era5_files, combine="by_coords", chunks={"time": 10}, engine="netcdf4"  # æŒ‰æ—¶é—´åˆ†å—å¤„ç†
        )

        # é‡å‘½åERA5æ•°æ®çš„ç»´åº¦ä»¥ä¾¿åç»­å¤„ç†
        self.era5_ds = self.era5_ds.rename({"valid_time": "time", "latitude": "lat", "longitude": "lon"})
        # è½¬æ¢æ•°æ®ç±»å‹ä¸ºfloat32ï¼Œå‡å°‘å†…å­˜å ç”¨
        if self.era5_ds["tp"].dtype != np.float32:
            self.era5_ds["tp"] = self.era5_ds["tp"].astype(np.float32)
        print(f"ERA5æ•°æ®æ—¶é—´èŒƒå›´: {self.era5_ds.time.values[0]} åˆ° {self.era5_ds.time.values[-1]}")

        # 3. å»¶è¿ŸåŠ è½½DEMæ•°æ®
        # ä»…å­˜å‚¨è·¯å¾„ï¼Œåœ¨éœ€è¦æ—¶å†åŠ è½½
        self.dem_data = self.preprocess_dem()
        print("DEMæ•°æ®å·²åŠ è½½")

        # ç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨ç›¸åŒçš„æ—¶é—´èŒƒå›´å†…
        common_times = np.intersect1d(self.station_ds.time, self.era5_ds.time)
        self.station_ds = self.station_ds.sel(time=common_times)
        self.era5_ds = self.era5_ds.sel(time=common_times)
        print(f"å…±åŒæ—¶é—´ç‚¹æ•°é‡: {len(common_times)}")

        # æå‰æå–ç«™ç‚¹ä½ç½®ä¿¡æ¯ï¼Œé¿å…é‡å¤è®¡ç®—
        self.station_lats = self.station_ds["lat"].values
        self.station_lons = self.station_ds["lon"].values

        # é¢„è®¡ç®—æ¯ä¸ªç«™ç‚¹å¯¹åº”çš„ERA5ç½‘æ ¼ç‚¹ç´¢å¼•ï¼Œé¿å…é‡å¤è®¡ç®—
        print("é¢„è®¡ç®—ç«™ç‚¹å¯¹åº”çš„ERA5ç½‘æ ¼ç‚¹ç´¢å¼•...")
        era5_lats = self.era5_ds.lat.values
        era5_lons = self.era5_ds.lon.values

        self.station_era5_lat_indices = []
        self.station_era5_lon_indices = []

        for i in range(len(self.station_lats)):
            lat_idx = np.abs(era5_lats - self.station_lats[i]).argmin()
            lon_idx = np.abs(era5_lons - self.station_lons[i]).argmin()
            self.station_era5_lat_indices.append(lat_idx)
            self.station_era5_lon_indices.append(lon_idx)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿æ›´é«˜æ•ˆåœ°ç´¢å¼•
        self.station_era5_lat_indices = np.array(self.station_era5_lat_indices)
        self.station_era5_lon_indices = np.array(self.station_era5_lon_indices)

        # é¢„è®¡ç®—ç«™ç‚¹é«˜ç¨‹ï¼Œé¿å…é‡å¤è®¡ç®—
        print("é¢„è®¡ç®—ç«™ç‚¹é«˜ç¨‹...")
        self.station_dem = np.zeros(len(self.station_lats), dtype=np.float32)
        for i in range(len(self.station_lats)):
            lat = self.station_lats[i]
            lon = self.station_lons[i]
            lat_idx = np.abs(self.dem_data.lat.values - lat).argmin()
            lon_idx = np.abs(self.dem_data.lon.values - lon).argmin()
            self.station_dem[i] = self.dem_data.values[lat_idx, lon_idx]

        # å¤„ç†ç«™ç‚¹é«˜ç¨‹ä¸­å¯èƒ½çš„NaNå€¼
        nan_mask = np.isnan(self.station_dem)
        if nan_mask.any():
            print(f"âš ï¸ ç«™ç‚¹é«˜ç¨‹æ•°æ®ä¸­å­˜åœ¨ {nan_mask.sum()} ä¸ªNaNå€¼ï¼Œå°†è¿›è¡Œå¡«å……")
            self.station_dem[nan_mask] = np.nanmean(self.station_dem[~nan_mask])

        print(f"ç«™ç‚¹é«˜ç¨‹èŒƒå›´: {np.nanmin(self.station_dem):.1f}m - {np.nanmax(self.station_dem):.1f}m")

    def preprocess_dem(self):
        """
        æŒ‰éœ€åŠ è½½å’Œé¢„å¤„ç†DEMæ•°æ®ï¼Œç¡®ä¿ä¸ERA5æ ¼ç‚¹ä¸€è‡´

        å‚æ•°:
        thread_id: çº¿ç¨‹/è¿›ç¨‹IDï¼Œç”¨äºåœ¨å¹¶è¡Œç¯å¢ƒä¸­åŒºåˆ†è¾“å‡º

        è¿”å›:
        å¤„ç†åçš„DEMæ•°æ®
        """

        print(f"ğŸ“Š åŠ è½½DEMæ•°æ®...")
        try:
            dem_ds = xr.open_dataset(self.dem_file)
        except Exception as e:
            print(f"âŒ åŠ è½½DEMæ•°æ®å¤±è´¥: {e}")
            return None

        print(f"ğŸ“Š å¯¹DEMæ•°æ®è¿›è¡Œé‡é‡‡æ ·åˆ°ERA5ç½‘æ ¼...")
        try:
            # å°†DEMæ•°æ®æ’å€¼åˆ°ERA5æ ¼ç‚¹ï¼Œå¹¶è½¬æ¢ä¸ºfloat32
            dem_regrid = (
                dem_ds["Band1"].interp(lat=self.era5_ds.lat, lon=self.era5_ds.lon, method="linear").astype(np.float32)
            )

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨NaNå€¼
            nan_count = np.isnan(dem_regrid.values).sum()
            if nan_count > 0:
                print(f"âš ï¸ DEMæ•°æ®ä¸­å­˜åœ¨ {nan_count} ä¸ªNaNå€¼ï¼Œå°†è¿›è¡Œå¡«å……")
                # ä½¿ç”¨æ’å€¼å¡«å……NaNå€¼

                dem_array = dem_regrid.values
                mask = np.isnan(dem_array)
                dem_array[mask] = ndimage.gaussian_filter(np.where(~mask, dem_array, 0), sigma=3)[mask]
                dem_regrid.values = dem_array

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨Infå€¼
            inf_count = np.isinf(dem_regrid.values).sum()
            if inf_count > 0:
                print(f"âš ï¸ DEMæ•°æ®ä¸­å­˜åœ¨ {inf_count} ä¸ªInfå€¼ï¼Œå°†æ›¿æ¢ä¸ºæœ‰æ•ˆå€¼")
                # æ›¿æ¢Infå€¼
                dem_regrid.values[np.isinf(dem_regrid.values)] = np.nanmean(
                    dem_regrid.values[~np.isinf(dem_regrid.values)]
                )

            print(
                f"âœ… DEMæ•°æ®å¤„ç†å®Œæˆï¼Œé«˜ç¨‹èŒƒå›´: {np.nanmin(dem_regrid.values):.1f}m - {np.nanmax(dem_regrid.values):.1f}m"
            )

            dem_ds.close()

        except Exception as e:
            print(f"âŒ é‡é‡‡æ ·DEMæ•°æ®å¤±è´¥: {e}")
            return None

        return dem_regrid

    def idw_interpolation(
        self,
        points,
        values,
        xi,
        yi,
        p=2,
        epsilon=1e-10,
        min_points=3,
        search_radius=None,
        max_points=None,
        batch_size=5000,
    ):
        # ç¡®ä¿è¾“å…¥ä¸ºnumpyæ•°ç»„
        points = np.asarray(points, dtype=np.float32)
        values = np.asarray(values, dtype=np.float32)

        # è¿‡æ»¤æ— æ•ˆå€¼
        mask = ~np.isnan(values)
        if not np.all(mask):
            points = points[mask]
            values = values[mask]

        # Numbaå‚æ•°é¢„å¤„ç† - å¤„ç†Noneå€¼
        numba_search_radius = 0.0 if search_radius is None else float(search_radius)
        numba_max_points = 0 if max_points is None else int(max_points)

        # æ£€æŸ¥æœ‰æ•ˆç«™ç‚¹æ•°é‡
        if len(points) < min_points:
            # ç«™ç‚¹æ•°ä¸è¶³ï¼Œè¿”å›å‡å€¼å¡«å……ç»“æœ
            print(f"è­¦å‘Š: æœ‰æ•ˆç«™ç‚¹æ•°é‡({len(points)})å°äºæœ€å°è¦æ±‚({min_points})ï¼Œä½¿ç”¨å‡å€¼å¡«å……")
            return np.ones_like(xi) * np.mean(values)

        # åˆ›å»ºç½‘æ ¼ç‚¹
        grid_points = np.column_stack([yi.flatten(), xi.flatten()]).astype(np.float32)
        zi = np.zeros(grid_points.shape[0], dtype=np.float32)

        # åˆ†æ‰¹å¤„ç†ç½‘æ ¼ç‚¹
        for i in range(0, grid_points.shape[0], batch_size):
            end_idx = min(i + batch_size, grid_points.shape[0])
            batch_points = grid_points[i:end_idx]

            # è°ƒç”¨NumbaåŠ é€Ÿçš„IDWå†…æ ¸
            result_batch = idw_numba_kernel(
                batch_points, points, values, p, epsilon, numba_search_radius, min_points, numba_max_points
            )

            # ä¿å­˜ç»“æœ
            zi[i:end_idx] = result_batch

        return zi.reshape(xi.shape)

    def mprism_interpolation_aligned_grid(
        self, points, values, xi, yi, n_regions=5, min_points_per_region=3, batch_size=100, verbose=False
    ):
        """
        MPRISMæ®‹å·®æ’å€¼ - ä½¿ç”¨NumbaåŠ é€Ÿçš„ä¼˜åŒ–ç‰ˆæœ¬
        é’ˆå¯¹dem_dataä¸ç›®æ ‡ç½‘æ ¼å®Œå…¨ä¸€è‡´çš„æƒ…å†µï¼Œä½¿ç”¨NumbaåŠ é€Ÿè®¡ç®—

        å‚æ•°:
        points: ç«™ç‚¹åæ ‡æ•°ç»„ [lat, lon]
        values: ç«™ç‚¹æ®‹å·®å€¼æ•°ç»„
        xi, yi: ç›®æ ‡ç½‘æ ¼åæ ‡
        n_regions: é«˜ç¨‹åˆ†åŒºæ•°é‡
        min_points_per_region: æ¯ä¸ªåˆ†åŒºæœ€å°‘ç«™ç‚¹æ•°
        batch_size: æ¯æ‰¹å¤„ç†çš„è¡Œæ•°ï¼Œæ§åˆ¶å†…å­˜ä½¿ç”¨
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        # åŸºç¡€æ•°æ®å‡†å¤‡
        points = np.asarray(points, dtype=np.float32)
        values = np.asarray(values, dtype=np.float32)
        mask = ~np.isnan(values)

        # æœ‰æ•ˆç‚¹æ£€æŸ¥
        if mask.sum() < min_points_per_region * 2:
            if verbose:
                print("æœ‰æ•ˆç‚¹å¤ªå°‘ï¼Œåˆ‡æ¢åˆ°IDWæ’å€¼...")
            return self.idw_interpolation(points[mask], values[mask], xi, yi)

        valid_points = points[mask]
        valid_values = values[mask]
        station_dem = self.station_dem[mask]

        # åˆ†åŒºå’Œæ¨¡å‹è®­ç»ƒéƒ¨åˆ†
        dem_min, dem_max = station_dem.min(), station_dem.max()
        dem_range = dem_max - dem_min
        if dem_range < 1:
            if verbose:
                print("é«˜ç¨‹å·®è¿‡å°ï¼Œåˆ‡æ¢åˆ°IDWæ’å€¼")
            return self.idw_interpolation(valid_points, valid_values, xi, yi)

        region_boundaries = np.linspace(dem_min, dem_max, n_regions + 1)

        # ä¸ºNumbaå‡½æ•°å‡†å¤‡æ•°æ®ç»“æ„
        region_coefs = np.zeros(n_regions, dtype=np.float32)
        region_intercepts = np.zeros(n_regions, dtype=np.float32)
        has_model = np.zeros(n_regions, dtype=np.bool_)

        # ä¸ºæ¯ä¸ªåˆ†åŒºå»ºæ¨¡ - æå–æ¨¡å‹ç³»æ•°ä¸ºNumbaå‡†å¤‡
        for r in range(n_regions):
            if r == n_regions - 1:
                region_mask = (station_dem >= region_boundaries[r]) & (station_dem <= region_boundaries[r + 1])
            else:
                region_mask = (station_dem >= region_boundaries[r]) & (station_dem < region_boundaries[r + 1])

            if np.sum(region_mask) < min_points_per_region:
                continue

            elev = station_dem[region_mask]
            vals = valid_values[region_mask]
            if len(elev) <= 1:
                continue

            model = LinearRegression()
            model.fit(elev.reshape(-1, 1), vals)

            # å­˜å‚¨ç³»æ•°ä¸ºNumbaå¯ç”¨æ ¼å¼
            region_coefs[r] = model.coef_[0]
            region_intercepts[r] = model.intercept_
            has_model[r] = True

        # åˆå§‹åŒ–ç»“æœæ•°ç»„
        result = np.zeros_like(xi, dtype=np.float32)

        # ç›´æ¥è·å–DEMæ•°æ®
        grid_elevs = self.dem_data.values

        # é¢„è®¡ç®—æ¯ä¸ªåˆ†åŒºæ¨¡å‹å¯¹æ‰€æœ‰ç«™ç‚¹çš„æ®‹å·®
        n_stations = len(valid_values)
        station_residuals = np.zeros((n_regions, n_stations), dtype=np.float32)

        for r in range(n_regions):
            if has_model[r]:
                preds = region_coefs[r] * station_dem + region_intercepts[r]
                station_residuals[r] = valid_values - preds

        # æå–ç«™ç‚¹åæ ‡
        valid_lats = valid_points[:, 0]  # ç¬¬ä¸€åˆ—æ˜¯çº¬åº¦
        valid_lons = valid_points[:, 1]  # ç¬¬äºŒåˆ—æ˜¯ç»åº¦

        # æŒ‰è¡Œåˆ†æ‰¹å¤„ç†ç½‘æ ¼ç‚¹
        height, width = xi.shape
        for i in range(0, height, batch_size):
            i_end = min(i + batch_size, height)

            # æ‰¹é‡è·å–å½“å‰è¡Œå—çš„é«˜ç¨‹å’ŒåŒºåŸŸ
            block_elevs = grid_elevs[i:i_end, :]
            block_regions = np.searchsorted(region_boundaries[1:], block_elevs)
            block_regions = np.minimum(block_regions, n_regions - 1)

            # è·å–å½“å‰æ‰¹æ¬¡çš„ç½‘æ ¼ç‚¹åæ ‡
            block_lats = yi[i:i_end, :]
            block_lons = xi[i:i_end, :]

            # é¢„åˆ†é…å½“å‰æ‰¹æ¬¡çš„ç»“æœæ•°ç»„
            block_result = np.zeros_like(block_elevs, dtype=np.float32)

            # é€åˆ—å¤„ç†ï¼Œä½¿ç”¨NumbaåŠ é€Ÿå‡½æ•°
            for j in range(width):
                # æå–å½“å‰åˆ—
                col_lats = block_lats[:, j]
                col_lons = block_lons[:, j]
                col_elevs = block_elevs[:, j]
                col_regions = block_regions[:, j]

                # ä½¿ç”¨NumbaåŠ é€Ÿçš„å‡½æ•°å¤„ç†æ•´åˆ—
                col_result = process_grid_column(
                    col_lats,
                    col_lons,
                    col_elevs,
                    col_regions,
                    valid_lats,
                    valid_lons,
                    valid_values,
                    region_coefs,
                    region_intercepts,
                    station_residuals,
                    n_regions,
                    has_model,
                )

                block_result[:, j] = col_result

            # å°†å½“å‰æ‰¹æ¬¡ç»“æœå†™å…¥æœ€ç»ˆç»“æœæ•°ç»„
            result[i:i_end, :] = block_result

        # ç¡®ä¿éè´Ÿå€¼
        result[result < 0] = 0
        return result

    def process_one_time_compute(self, t_idx, method="mprism", use_dem=True, idw_params=None):
        """å•ä¸ªæ—¶é—´ç‚¹çš„æ’å€¼æ“ä½œï¼Œåªè®¡ç®—ä¸å†™å…¥ï¼Œè¿”å›è®¡ç®—ç»“æœ"""
        # try:
        # é»˜è®¤IDWå‚æ•°
        if idw_params is None:
            idw_params = {
                "p": 2,  # å¹‚æŒ‡æ•°
                "epsilon": 1e-10,  # é˜²é™¤é›¶
                "min_points": 3,  # æœ€å°‘ç«™ç‚¹æ•°
                "search_radius": None,  # æœç´¢åŠå¾„
                "batch_size": 5000,  # æ‰¹å¤„ç†å¤§å°
            }

        current_time = self.station_ds.time.values[t_idx]
        era5_precip = self.era5_ds["tp"].sel(time=current_time).values.astype(np.float32)
        station_precip = self.station_ds["rain1h_qc"].sel(time=current_time).values

        # ç­›é€‰é™æ°´å€¼å¤§äº0çš„ç«™ç‚¹
        rain_mask = station_precip > 0.1

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå¤šçš„é™æ°´ç«™ç‚¹
        if np.sum(rain_mask) < idw_params["min_points"]:
            print(f"è­¦å‘Š: æ—¶é—´ç‚¹ {t_idx} çš„é™æ°´ç«™ç‚¹æ•°é‡ä¸è¶³ ({np.sum(rain_mask)}/{len(station_precip)})")
            # å¦‚æœé™æ°´ç«™ç‚¹å¤ªå°‘ï¼Œå¯é€‰æ‹©ä»¥ä¸‹ä¸¤ç§å¤„ç†æ–¹å¼ï¼š

            # é€‰é¡¹1: ä»ä½¿ç”¨æ‰€æœ‰ç«™ç‚¹ï¼ŒåŒ…æ‹¬é›¶é™æ°´ç«™ç‚¹
            # rain_mask = np.ones_like(rain_mask, dtype=bool)

            # é€‰é¡¹2: ç›´æ¥ä½¿ç”¨ERA5æ•°æ®ï¼Œä¸è¿›è¡Œç«™ç‚¹è®¢æ­£
            return {
                "t_idx": t_idx,
                "time_str": pd.Timestamp(current_time).strftime("%Y-%m-%d %H:%M"),
                "precip": era5_precip,
                "status": "success",
                "note": "ä½¿ç”¨åŸå§‹ERA5æ•°æ®ï¼ˆé™æ°´ç«™ç‚¹ä¸è¶³ï¼‰",
            }

        # è·å–æœ‰é™æ°´çš„ç«™ç‚¹çš„ä½ç½®å’ŒERA5å€¼
        rainy_station_lats = self.station_lats[rain_mask]
        rainy_station_lons = self.station_lons[rain_mask]
        rainy_station_precip = station_precip[rain_mask]
        rainy_station_points = np.column_stack([rainy_station_lats, rainy_station_lons])

        # è·å–å¯¹åº”çš„ERA5å€¼
        rainy_era5_at_stations = era5_precip[
            self.station_era5_lat_indices[rain_mask], self.station_era5_lon_indices[rain_mask]
        ]

        # è®¡ç®—æ®‹å·®
        residuals = rainy_station_precip - rainy_era5_at_stations

        # åˆ›å»ºç½‘æ ¼ç‚¹
        grid_lon, grid_lat = np.meshgrid(self.era5_ds.lon.values, self.era5_ds.lat.values)

        # å¦‚æœä½¿ç”¨DEMï¼Œè¿˜éœ€è¦ç­›é€‰å¯¹åº”çš„ç«™ç‚¹é«˜ç¨‹æ•°æ®
        if method == "mprism" and use_dem:
            # ç­›é€‰é™æ°´ç«™ç‚¹å¯¹åº”çš„DEMæ•°æ®
            rainy_station_dem = self.station_dem[rain_mask]

            # ä¿å­˜åŸå§‹ç«™ç‚¹DEM
            original_station_dem = self.station_dem

            # ä¸´æ—¶æ›¿æ¢ä¸ºåªåŒ…å«é™æ°´ç«™ç‚¹çš„DEMå€¼
            self.station_dem = rainy_station_dem

            # æ‰§è¡Œæ’å€¼
            residual_grid = self.mprism_interpolation_aligned_grid(rainy_station_points, residuals, grid_lon, grid_lat)

            # æ¢å¤åŸå§‹ç«™ç‚¹DEM
            self.station_dem = original_station_dem
        else:
            # ä½¿ç”¨IDWæ’å€¼ï¼ŒåªåŸºäºæœ‰é™æ°´çš„ç«™ç‚¹
            residual_grid = self.idw_interpolation(
                rainy_station_points,
                residuals,
                grid_lon,
                grid_lat,
                p=idw_params["p"],
                epsilon=idw_params["epsilon"],
                min_points=idw_params["min_points"],
                search_radius=idw_params["search_radius"],
                max_points=idw_params["max_points"],
                batch_size=idw_params["batch_size"],
            )

        final_precip = era5_precip + residual_grid
        final_precip[final_precip < 0] = 0

        # è¿”å›è®¡ç®—ç»“æœ
        time_str = pd.Timestamp(current_time).strftime("%Y-%m-%d %H:%M")
        return {
            "t_idx": t_idx,
            "time_str": time_str,
            "precip": final_precip,
            "status": "success",
            "used_stations": np.sum(rain_mask),
        }
        # except Exception as e:
        #     return {"t_idx": t_idx, "status": "failed", "error": str(e)}

    def process_all_times_parallel(self, method="mprism", use_dem=True, resume=True, zarr_name="temp_output.zarr"):
        """
        ä½¿ç”¨ Joblib å¹¶è¡Œå¤„ç†æ‰€æœ‰æ—¶é—´ç‚¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 

        å‚æ•°:
        method: æ’å€¼æ–¹æ³•ï¼Œ'mprism'æˆ–'idw'
        use_dem: æ˜¯å¦ä½¿ç”¨DEMæ•°æ®
        resume: æ˜¯å¦æ–­ç‚¹ç»­ä¼ 
        zarr_name: è¾“å‡ºzarræ–‡ä»¶å
        idw_params: IDWæ’å€¼å‚æ•°å­—å…¸
        """
        # è‡ªåŠ¨è®¡ç®—é€‚åˆçš„æœç´¢åŠå¾„ (åŸºäºç«™ç‚¹ç©ºé—´åˆ†å¸ƒ)
        station_points = np.column_stack([self.station_lats, self.station_lons])
        radius = self.calculate_adaptive_radius(station_points, min_points=5)
        print(f"è®¡ç®—çš„è‡ªé€‚åº”æœç´¢åŠå¾„: {radius:.4f}")

        idw_params = {
            "p": 2,
            "epsilon": 1e-10,
            "min_points": 3,
            "search_radius": radius,
            "max_points": None,
            "batch_size": 5000,
        }

        # åˆ›å»º zarr å­˜å‚¨è·¯å¾„
        zarr_path = os.path.join(self.output_folder, zarr_name)
        # åˆ›å»ºè·Ÿè¸ªæ–‡ä»¶è·¯å¾„
        tracking_file = os.path.join(self.output_folder, f"{zarr_name.split('.')[0]}_processed_times.json")

        if not os.path.exists(zarr_path):
            # æ„å»ºåˆå§‹ç©º DataArray
            ds_template = xr.Dataset(
                data_vars={
                    "corrected_precip": (
                        ("time", "lat", "lon"),
                        np.zeros(
                            (
                                len(self.station_ds.time),
                                len(self.era5_ds.lat),
                                len(self.era5_ds.lon),
                            ),
                            dtype=np.float32,
                        ),
                    )
                },
                coords={
                    "time": self.station_ds.time.values,
                    "lat": self.era5_ds.lat.values,
                    "lon": self.era5_ds.lon.values,
                },
            )
            ds_template.to_zarr(zarr_path, mode="w")

            # åˆ›å»ºæ–°çš„è·Ÿè¸ªæ–‡ä»¶
            with open(tracking_file, "w") as f:
                json.dump({"processed_indices": []}, f)

        # æ–­ç‚¹ç»­ä¼ ï¼šä»è·Ÿè¸ªæ–‡ä»¶è¯»å–å·²å¤„ç†çš„æ—¶é—´ç‚¹
        processed_indices = []
        if resume and os.path.exists(tracking_file):
            try:
                with open(tracking_file, "r") as f:
                    tracking_data = json.load(f)
                    processed_indices = tracking_data.get("processed_indices", [])
                print(f"ä»è·Ÿè¸ªæ–‡ä»¶åŠ è½½å·²å¤„ç†æ—¶é—´ç‚¹: {len(processed_indices)}")
            except Exception as e:
                print(f"è¯»å–è·Ÿè¸ªæ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°åˆ›å»º")
                processed_indices = []

        all_indices = set(range(len(self.station_ds.time)))
        remain_indices = list(all_indices - set(processed_indices))
        total_tasks = len(remain_indices)
        print(f"å·²å¤„ç†æ—¶é—´ç‚¹æ•°é‡: {len(processed_indices)}, å‰©ä½™: {total_tasks}")

        if len(remain_indices) == 0:
            print("æ‰€æœ‰æ—¶é—´ç‚¹å‡å·²å¤„ç†ï¼Œæ— éœ€é‡å¤è¿è¡Œã€‚")
            return xr.open_zarr(zarr_path)

        # ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œä½†è¿›åº¦æ¡ç•™åœ¨ä¸»è¿›ç¨‹
        print("å¼€å§‹å¹¶è¡Œå¤„ç†...")
        with parallel_backend("loky", n_jobs=4):  # å‡å°‘å¹¶è¡Œåº¦ï¼Œé¿å…è¿‡å¤šè¿›ç¨‹äº‰ç”¨èµ„æº
            # åˆå§‹åŒ–è¿›åº¦æ¡ä½†ä¸ä¼ é€’ç»™å­è¿›ç¨‹
            with tqdm(total=total_tasks, desc="å¤„ç†è¿›åº¦", unit="æ—¶é—´ç‚¹") as progress:
                # å°†ä»»åŠ¡åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»ºè¿‡å¤šè¿›ç¨‹
                batch_size = min(24, total_tasks)  # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œå‡è½»å†…å­˜å‹åŠ›
                for i in range(0, total_tasks, batch_size):
                    end_idx = min(i + batch_size, total_tasks)
                    batch_indices = remain_indices[i:end_idx]

                    batch_results = Parallel()(
                        delayed(self.process_one_time_compute)(t_idx, method, use_dem, idw_params)
                        for t_idx in batch_indices
                    )

                    # è·å–æˆåŠŸè®¡ç®—çš„ç»“æœ
                    successful_results = [r for r in batch_results if r["status"] == "success"]
                    failed_results = [r for r in batch_results if r["status"] == "failed"]

                    # åœ¨ä¸»è¿›ç¨‹ä¸­é›†ä¸­å†™å…¥ç»“æœåˆ°Zarr - é¿å…å¹¶å‘å†™å…¥
                    if successful_results:
                        # è¯»å–å·²æœ‰çš„ Zarr æ•°æ®é›†
                        ds_zarr = xr.open_zarr(zarr_path)

                        # æ›´æ–°æ•°æ®
                        for result in successful_results:
                            t_idx = result["t_idx"]
                            # æ›´æ–°å†…å­˜ä¸­çš„æ•°æ®æ•°ç»„
                            ds_zarr["corrected_precip"][t_idx] = result["precip"]

                        # é‡æ–°å†™å…¥ Zarrï¼Œä½¿ç”¨ mode='a' è¿½åŠ æ¨¡å¼
                        # ä½¿ç”¨ region å‚æ•°æŒ‡å®šåªå†™å…¥å·²ç»ä¿®æ”¹çš„åŒºåŸŸ
                        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† consolidated=True ç¡®ä¿å…ƒæ•°æ®è¢«æ­£ç¡®å¤„ç†
                        for result in successful_results:
                            t_idx = result["t_idx"]
                            # æå–å•ä¸ªæ—¶é—´ç‚¹çš„åˆ‡ç‰‡
                            ds_slice = ds_zarr.isel(time=[t_idx])
                            # å†™å…¥è¿™ä¸ªæ—¶é—´ç‚¹ï¼Œä½¿ç”¨ append_dim=None è¡¨ç¤ºè¦†ç›–è€Œéè¿½åŠ 
                            ds_slice.to_zarr(
                                zarr_path,
                                region={"time": slice(t_idx, t_idx + 1), "lat": slice(None), "lon": slice(None)},
                                consolidated=True,
                            )

                        # ç¡®ä¿èµ„æºé‡Šæ”¾
                        ds_zarr = None

                        # æ‰¹é‡æ›´æ–°è·Ÿè¸ªæ–‡ä»¶
                        processed_batch_indices = [r["t_idx"] for r in successful_results]
                        lock_file = tracking_file + ".lock"
                        with FileLock(lock_file):
                            with open(tracking_file, "r") as f:
                                tracking_data = json.load(f)

                            # æ›´æ–°å·²å¤„ç†ç´¢å¼•
                            tracking_data["processed_indices"].extend(processed_batch_indices)

                            # è®°å½•å¤„ç†æ—¶é—´ï¼ˆå¯é€‰ï¼‰
                            if "processing_times" not in tracking_data:
                                tracking_data["processing_times"] = {}

                            for result in successful_results:
                                tracking_data["processing_times"][str(result["t_idx"])] = result.get("process_time", 0)

                            with open(tracking_file, "w") as f:
                                json.dump(tracking_data, f)

                    # æ›´æ–°è¿›åº¦æ¡
                    for result in batch_results:
                        progress.update(1)
                        if result["status"] == "success":
                            progress.set_postfix({"å½“å‰å¤„ç†": result["time_str"]})
                        else:
                            progress.write(f"æ—¶é—´ç‚¹ {result['t_idx']} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    # è¾“å‡ºæ‰¹æ¬¡å¤„ç†ç»Ÿè®¡
                    if failed_results:
                        print(
                            f"æ‰¹æ¬¡ {i//batch_size + 1}: æˆåŠŸ {len(successful_results)}/{len(batch_results)}ï¼Œ"
                            f"å¤±è´¥ {len(failed_results)}"
                        )

                    # æ‰§è¡Œåƒåœ¾å›æ”¶ï¼Œå‡è½»å†…å­˜å‹åŠ›
                    import gc

                    gc.collect()

        # ç»Ÿè®¡å¤„ç†ç»“æœ
        processed_indices = []
        with open(tracking_file, "r") as f:
            tracking_data = json.load(f)
            processed_indices = tracking_data.get("processed_indices", [])

        print(f"å¤„ç†å®Œæˆï¼šæˆåŠŸ {len(processed_indices)}/{len(all_indices)} ä¸ªæ—¶é—´ç‚¹")
        print("å¹¶è¡Œæ’å€¼å¤„ç†å®Œæˆï¼ŒZarr æ•°æ®å†™å…¥å®Œæˆã€‚")
        print(f"Zarr æ–‡ä»¶è·¯å¾„: {zarr_path}")
        print(f"å¤„ç†è¿›åº¦è·Ÿè¸ªæ–‡ä»¶: {tracking_file}")

        return xr.open_zarr(zarr_path)

    def validate_results(self, corrected_ds, method="idw", use_dem=True, min_points=3, min_precip=0.1):
        """
        éªŒè¯æ’å€¼ç»“æœï¼Œé‡‡ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯ - ä½¿ç”¨é¢„è®¡ç®—çš„ERA5ç½‘æ ¼ç‚¹ç´¢å¼•ï¼Œæ”¯æŒå¤šç§æ’å€¼æ–¹æ³•

        å‚æ•°:
        corrected_ds: å·²è®¢æ­£çš„æ•°æ®é›†ï¼ˆå®é™…æœªä½¿ç”¨ï¼Œä»…ä¸ºæ¥å£ä¸€è‡´æ€§ï¼‰
        method: æ’å€¼æ–¹æ³•ï¼Œæ”¯æŒ'idw'å’Œ'mprism'
        use_dem: æ˜¯å¦ä½¿ç”¨DEMæ•°æ®
        min_points: æœ€å°‘éœ€è¦çš„ç«™ç‚¹æ•°
        min_precip: æœ€å°æœ‰æ•ˆé™æ°´å€¼

        è¿”å›:
        éªŒè¯ç»“æœDataFrame
        """
        print(f"å¼€å§‹éªŒè¯ç»“æœ (æ–¹æ³•: {method}, ä½¿ç”¨DEM: {use_dem})...")

        # é€‰æ‹©å¤šä¸ªæ—¶é—´ç‚¹è¿›è¡ŒéªŒè¯ï¼Œå¢åŠ éªŒè¯çš„ç¨³å®šæ€§
        total_times = len(self.station_ds.time)
        time_indices = [total_times // 4, total_times // 2, total_times * 3 // 4]  # é€‰æ‹©ä¸‰ä¸ªæ—¶é—´ç‚¹

        # åˆå§‹åŒ–æ‰€æœ‰éªŒè¯ç»“æœ
        all_validation_results = []

        for time_idx in time_indices:
            current_time = self.station_ds.time.values[time_idx]
            time_str = pd.Timestamp(current_time).strftime("%Y-%m-%d %H:%M")
            print(f"éªŒè¯æ—¶é—´ç‚¹: {time_str}")

            # æå–ç«™ç‚¹ä½ç½®å’Œé™æ°´
            station_lats = self.station_lats
            station_lons = self.station_lons
            station_precip = self.station_ds["rain1h_qc"].sel(time=current_time).values

            # è·å–å½“å‰æ—¶é—´çš„ERA5æ•°æ®
            era5_time = self.era5_ds["tp"].sel(time=current_time).values

            # ç­›é€‰æœ‰æ•ˆé™æ°´ç«™ç‚¹ï¼ˆä¸process_one_time_computeä¿æŒä¸€è‡´ï¼‰
            rain_mask = station_precip > min_precip
            if np.sum(rain_mask) < min_points + 1:  # +1æ˜¯å› ä¸ºç•™ä¸€æ³•ä¼šå‡å°‘ä¸€ä¸ªç«™ç‚¹
                print(f"è­¦å‘Š: æ—¶é—´ç‚¹ {time_str} çš„æœ‰æ•ˆé™æ°´ç«™ç‚¹æ•°é‡ä¸è¶³ ({np.sum(rain_mask)}/{len(station_precip)})")
                continue  # è·³è¿‡æ­¤æ—¶é—´ç‚¹

            # åˆå§‹åŒ–æ­¤æ—¶é—´ç‚¹çš„éªŒè¯ç»“æœ
            validation_results = []

            # åˆ›å»ºç½‘æ ¼ç‚¹ï¼ˆç”¨äºMPRISMæ–¹æ³•ï¼‰
            grid_lon, grid_lat = np.meshgrid(self.era5_ds.lon.values, self.era5_ds.lat.values)

            # å¯¹æ¯ä¸ªæœ‰æ•ˆé™æ°´ç«™ç‚¹è¿›è¡Œç•™ä¸€æ³•äº¤å‰éªŒè¯
            for i in range(len(station_lats)):
                if not rain_mask[i]:
                    continue  # è·³è¿‡æ— é™æ°´ç«™ç‚¹

                # æ’é™¤å½“å‰ç«™ç‚¹
                leave_one_mask = rain_mask.copy()
                leave_one_mask[i] = False

                # æ£€æŸ¥å‰©ä½™ç«™ç‚¹æ•°é‡
                if np.sum(leave_one_mask) < min_points:
                    continue  # ç«™ç‚¹æ•°ä¸è¶³ï¼Œè·³è¿‡è¯¥ç«™ç‚¹

                # æå–ç•™ä¸€åçš„ç«™ç‚¹æ•°æ®
                leave_one_lats = station_lats[leave_one_mask]
                leave_one_lons = station_lons[leave_one_mask]
                leave_one_precip = station_precip[leave_one_mask]
                leave_one_points = np.column_stack([leave_one_lats, leave_one_lons])

                # ä½¿ç”¨é¢„è®¡ç®—çš„ç´¢å¼•è·å–ERA5å€¼
                era5_at_stations = era5_time[
                    self.station_era5_lat_indices[leave_one_mask], self.station_era5_lon_indices[leave_one_mask]
                ]

                # è·å–æµ‹è¯•ç«™ç‚¹ä¿¡æ¯
                test_lat = station_lats[i]
                test_lon = station_lons[i]
                test_precip = station_precip[i]
                test_point = np.array([[test_lat, test_lon]], dtype=np.float32)

                # è·å–æµ‹è¯•ç«™ç‚¹çš„ERA5å€¼
                test_era5 = era5_time[self.station_era5_lat_indices[i], self.station_era5_lon_indices[i]]

                # è®¡ç®—æ®‹å·®
                residuals = leave_one_precip - era5_at_stations

                # å¯¹æµ‹è¯•ç«™ç‚¹è¿›è¡Œæ®‹å·®æ’å€¼
                if method == "mprism" and use_dem:
                    try:
                        # ä¿å­˜åŸå§‹ç«™ç‚¹DEMæ•°æ®
                        original_dem = self.station_dem

                        # ç­›é€‰ç•™ä¸€åçš„ç«™ç‚¹é«˜ç¨‹
                        leave_one_dem = self.station_dem[leave_one_mask]
                        # ä¸´æ—¶æ›¿æ¢ç«™ç‚¹DEMä¸ºç•™ä¸€åçš„å€¼
                        self.station_dem = leave_one_dem

                        # ä½¿ç”¨MPRISMæ–¹æ³•æ’å€¼ï¼ˆéœ€è¦ä¿®æ”¹ä¸ºå•ç‚¹æ’å€¼ï¼‰
                        # åˆ›å»ºåŒ…å«æµ‹è¯•ç‚¹çš„å°ç½‘æ ¼
                        test_grid_lat = np.array([[test_lat]])
                        test_grid_lon = np.array([[test_lon]])

                        # æ‰§è¡ŒMPRISMæ’å€¼
                        interpolated_residual = self.mprism_interpolation_aligned_grid(
                            leave_one_points, residuals, test_grid_lon, test_grid_lat, verbose=False
                        ).flatten()[0]

                        # æ¢å¤åŸå§‹ç«™ç‚¹DEM
                        self.station_dem = original_dem
                    except Exception as e:
                        print(f"ç«™ç‚¹ {i} MPRISMæ’å€¼å¤±è´¥ï¼Œå›é€€åˆ°IDW: {e}")
                        # ä½¿ç”¨IDWæ’å€¼ä½œä¸ºå¤‡é€‰
                        distances = np.sqrt((leave_one_lats - test_lat) ** 2 + (leave_one_lons - test_lon) ** 2)
                        if np.all(distances > 1e-10):  # é¿å…é™¤é›¶é”™è¯¯
                            weights = 1.0 / (distances**2 + 1e-10)
                            weights_sum = np.sum(weights)
                            interpolated_residual = np.sum(weights * residuals) / weights_sum
                        else:
                            # å¦‚æœæœ‰å®Œå…¨é‡åˆçš„ç‚¹ï¼Œç›´æ¥ä½¿ç”¨è¯¥ç‚¹çš„å€¼
                            idx = np.argmin(distances)
                            interpolated_residual = residuals[idx]
                else:
                    # ä½¿ç”¨IDWæ’å€¼
                    distances = np.sqrt((leave_one_lats - test_lat) ** 2 + (leave_one_lons - test_lon) ** 2)
                    if np.all(distances > 1e-10):  # é¿å…é™¤é›¶é”™è¯¯
                        weights = 1.0 / (distances**2 + 1e-10)
                        weights_sum = np.sum(weights)
                        interpolated_residual = np.sum(weights * residuals) / weights_sum
                    else:
                        # å¦‚æœæœ‰å®Œå…¨é‡åˆçš„ç‚¹ï¼Œç›´æ¥ä½¿ç”¨è¯¥ç‚¹çš„å€¼
                        idx = np.argmin(distances)
                        interpolated_residual = residuals[idx]

                # è®¡ç®—æ’å€¼é¢„æµ‹é™æ°´
                predicted_precip = test_era5 + interpolated_residual
                if predicted_precip < 0:
                    predicted_precip = 0

                # å­˜å‚¨ç»“æœ
                validation_results.append(
                    {
                        "time": time_str,
                        "station_id": self.station_ds.station.values[i],
                        "lat": test_lat,
                        "lon": test_lon,
                        "elev": self.station_dem[i],
                        "obs_precip": test_precip,
                        "era5_precip": test_era5,
                        "corrected_precip": predicted_precip,
                        "abs_error": abs(predicted_precip - test_precip),
                        "rel_error": abs(predicted_precip - test_precip) / (test_precip + 1e-5),
                        "era5_error": abs(test_era5 - test_precip),
                    }
                )

            # å°†æ­¤æ—¶é—´ç‚¹çš„ç»“æœæ·»åŠ åˆ°æ€»ç»“æœä¸­
            all_validation_results.extend(validation_results)

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç»“æœ
        if not all_validation_results:
            print("è­¦å‘Š: æœªèƒ½è¿›è¡Œæœ‰æ•ˆéªŒè¯ï¼Œå¯èƒ½æ˜¯ç”±äºå¯ç”¨ç«™ç‚¹å¤ªå°‘")
            return None

        # è½¬æ¢ä¸ºDataFrame
        validation_df = pd.DataFrame(all_validation_results)

        # è®¡ç®—è¯¦ç»†éªŒè¯æŒ‡æ ‡
        # 1. RMSE
        rmse_era5 = np.sqrt(mean_squared_error(validation_df["obs_precip"], validation_df["era5_precip"]))
        rmse_corrected = np.sqrt(mean_squared_error(validation_df["obs_precip"], validation_df["corrected_precip"]))

        # 2. MAE (å¹³å‡ç»å¯¹è¯¯å·®)
        mae_era5 = np.mean(np.abs(validation_df["obs_precip"] - validation_df["era5_precip"]))
        mae_corrected = np.mean(np.abs(validation_df["obs_precip"] - validation_df["corrected_precip"]))

        # 3. ç›¸å…³ç³»æ•°
        corr_era5 = np.corrcoef(validation_df["obs_precip"], validation_df["era5_precip"])[0, 1]
        corr_corrected = np.corrcoef(validation_df["obs_precip"], validation_df["corrected_precip"])[0, 1]

        # 4. é™æ°´é‡åˆ†ç»„æŒ‡æ ‡
        # å°é›¨ (0.1-10mm)ã€ä¸­é›¨ (10-25mm)ã€å¤§é›¨ (>25mm)
        light_mask = (validation_df["obs_precip"] > 0.1) & (validation_df["obs_precip"] <= 10)
        medium_mask = (validation_df["obs_precip"] > 10) & (validation_df["obs_precip"] <= 25)
        heavy_mask = validation_df["obs_precip"] > 25

        # æ‰“å°åˆ†ç»„ç»Ÿè®¡
        print("\n===== éªŒè¯ç»“æœæ€»è§ˆ =====")
        print(f"æ€»æ ·æœ¬æ•°: {len(validation_df)}")
        print(f"å°é›¨æ ·æœ¬æ•°: {np.sum(light_mask)}")
        print(f"ä¸­é›¨æ ·æœ¬æ•°: {np.sum(medium_mask)}")
        print(f"å¤§é›¨æ ·æœ¬æ•°: {np.sum(heavy_mask)}")

        # æ•´ä½“ç»Ÿè®¡
        print("\n===== æ•´ä½“ç»Ÿè®¡ =====")
        print(
            f"RMSE: ERA5 = {rmse_era5:.4f}, è®¢æ­£å = {rmse_corrected:.4f}, æ”¹è¿›ç‡ = {(rmse_era5-rmse_corrected)/rmse_era5*100:.2f}%"
        )
        print(
            f"MAE:  ERA5 = {mae_era5:.4f}, è®¢æ­£å = {mae_corrected:.4f}, æ”¹è¿›ç‡ = {(mae_era5-mae_corrected)/mae_era5*100:.2f}%"
        )
        print(f"ç›¸å…³ç³»æ•°: ERA5 = {corr_era5:.4f}, è®¢æ­£å = {corr_corrected:.4f}")

        # è®¡ç®—åˆ†ç»„RMSE
        if np.sum(light_mask) > 0:
            rmse_light_era5 = np.sqrt(
                mean_squared_error(
                    validation_df.loc[light_mask, "obs_precip"], validation_df.loc[light_mask, "era5_precip"]
                )
            )
            rmse_light_corrected = np.sqrt(
                mean_squared_error(
                    validation_df.loc[light_mask, "obs_precip"], validation_df.loc[light_mask, "corrected_precip"]
                )
            )
            print(f"\nå°é›¨ (0.1-10mm):")
            print(
                f"  RMSE: ERA5 = {rmse_light_era5:.4f}, è®¢æ­£å = {rmse_light_corrected:.4f}, æ”¹è¿›ç‡ = {(rmse_light_era5-rmse_light_corrected)/rmse_light_era5*100:.2f}%"
            )

        if np.sum(medium_mask) > 0:
            rmse_medium_era5 = np.sqrt(
                mean_squared_error(
                    validation_df.loc[medium_mask, "obs_precip"], validation_df.loc[medium_mask, "era5_precip"]
                )
            )
            rmse_medium_corrected = np.sqrt(
                mean_squared_error(
                    validation_df.loc[medium_mask, "obs_precip"], validation_df.loc[medium_mask, "corrected_precip"]
                )
            )
            print(f"\nä¸­é›¨ (10-25mm):")
            print(
                f"  RMSE: ERA5 = {rmse_medium_era5:.4f}, è®¢æ­£å = {rmse_medium_corrected:.4f}, æ”¹è¿›ç‡ = {(rmse_medium_era5-rmse_medium_corrected)/rmse_medium_era5*100:.2f}%"
            )

        if np.sum(heavy_mask) > 0:
            rmse_heavy_era5 = np.sqrt(
                mean_squared_error(
                    validation_df.loc[heavy_mask, "obs_precip"], validation_df.loc[heavy_mask, "era5_precip"]
                )
            )
            rmse_heavy_corrected = np.sqrt(
                mean_squared_error(
                    validation_df.loc[heavy_mask, "obs_precip"], validation_df.loc[heavy_mask, "corrected_precip"]
                )
            )
            print(f"\nå¤§é›¨ (>25mm):")
            print(
                f"  RMSE: ERA5 = {rmse_heavy_era5:.4f}, è®¢æ­£å = {rmse_heavy_corrected:.4f}, æ”¹è¿›ç‡ = {(rmse_heavy_era5-rmse_heavy_corrected)/rmse_heavy_era5*100:.2f}%"
            )

        # ä¿å­˜éªŒè¯ç»“æœ
        validation_file = os.path.join(
            self.output_folder, f'validation_{method}_{"with_dem" if use_dem else "no_dem"}.csv'
        )
        validation_df.to_csv(validation_file, index=False)
        print(f"\néªŒè¯ç»“æœå·²ä¿å­˜è‡³: {validation_file}")

        return validation_df

    def calculate_adaptive_radius(self, points, min_points=5):
        """
        è®¡ç®—è‡ªé€‚åº”æœç´¢åŠå¾„ï¼Œç¡®ä¿æ¯ä¸ªç½‘æ ¼ç‚¹è‡³å°‘æœ‰min_pointsä¸ªç«™ç‚¹

        å‚æ•°:
        points: ç«™ç‚¹åæ ‡æ•°ç»„
        min_points: æœ€å°‘ç«™ç‚¹æ•°é‡

        è¿”å›:
        æ¨èçš„æœç´¢åŠå¾„
        """
        from scipy.spatial import cKDTree

        # æ„å»ºKDæ ‘
        tree = cKDTree(points)

        # è®¡ç®—æ¯ä¸ªç«™ç‚¹åˆ°ç¬¬min_pointsä¸ªæœ€è¿‘é‚»ç«™ç‚¹çš„è·ç¦»
        all_dists = []
        for point in points:
            # æŸ¥è¯¢min_points+1ä¸ªç‚¹(åŒ…æ‹¬è‡ªèº«)
            dists, _ = tree.query(point, k=min(min_points + 1, len(points)))
            # å–æœ€è¿œçš„é‚£ä¸ªè·ç¦»
            if len(dists) > 1:
                all_dists.append(dists[-1])

        if not all_dists:
            # å¦‚æœç«™ç‚¹å¤ªå°‘ï¼Œä½¿ç”¨é»˜è®¤å€¼
            return 1.0

        # ä½¿ç”¨è·ç¦»çš„95%åˆ†ä½æ•°ä½œä¸ºæ¨èåŠå¾„
        radius = np.percentile(all_dists, 95)

        # å¢åŠ ä¸€äº›ä½™é‡
        return radius * 1.2


def main():
    """ä¸»å‡½æ•°"""
    # åŸºæœ¬é…ç½®
    station_file = "/mnt/h/DataSet/station_precipitation_data_filled.nc"
    era5_folder = "/mnt/h/DataSet/Pre_DEM"
    dem_file = "/mnt/h/DataSet/3-DEM/DEM_clip.nc"
    output_folder = "/mnt/h/DataSet/PreGrids_MPRISM"

    # åˆ›å»ºæ’å€¼å™¨å®ä¾‹
    interpolator = PrecipitationInterpolation(
        station_file=station_file, era5_folder=era5_folder, dem_file=dem_file, output_folder=output_folder
    )

    # æ‰§è¡Œæ’å€¼è®¡ç®—
    corrected_ds = interpolator.process_all_times_parallel(method="mprism", use_dem=True, resume=True)

    # éªŒè¯ç»“æœ
    # interpolator.validate_results(corrected_ds, method="mprism", use_dem=True)
    print("å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    # å¯¹äºå¤šè¿›ç¨‹ç¨‹åºï¼Œä¿æŠ¤å…¥å£ç‚¹å¾ˆé‡è¦
    main()
